#!/bin/bash
# desc : my commonly used functions

# ${jdbc_table} ${hive_table}
#function: import jdbc_data to hive_table
function import_data_hive() {
  if [ $# -ne 5 ] ; then
    echo_ex "args count $# is error!"
    exit 255
  fi
  jdbc_table=$1
  hive_table=$2

  jdbc_url=$3
  jdbc_username=$4
  jdbc_passwd=$5
  echo "start import hive. from ${jdbc_table}..."
  if [ -n "${jdbc_table}" ] ; then
    echo_ex "${HADOOP} fs -rmr /user/hdfs/${jdbc_table}"
    ${HADOOP} fs -rmr /user/hdfs/${jdbc_table}
  fi
  sqoop import \
    --connect ${jdbc_url} \
    --username ${jdbc_username} --password  ${jdbc_passwd} \
    --table ${jdbc_table} --fields-terminated-by "\001" --lines-terminated-by "\n" \
    --hive-import --hive-overwrite --hive-table ${hive_table} \
    --null-string '\\N' --null-non-string '\\N'
  check_success
  echo "end successful import to ${hive_table}"
}
# ${jdbc_table} ${hive_dir} ${hive_table} ${sql}
function import_data_query_hive() {
  if [ $# -lt 7 ] ; then
    echo_ex "args count $# is error!"
    exit 255
  fi
  jdbc_table=$1
  hive_dir=$2
  hive_table=$3
  sql=$4

  jdbc_url=$5
  jdbc_username=$6
  jdbc_passwd=$7
  id="id"
  if [ $# -gt 7 ] ; then
    id=$8
  fi
  
  echo_ex "${sql}"
  if [ -n "${hive_table}" ] && [ ${hive_table} != "/" ]; then
    ${HADOOP} fs -rmr "${hive_dir}${hive_table}"
  fi
  sqoop import \
    --connect ${jdbc_url} \
    --username ${jdbc_username} --password ${jdbc_passwd} \
    --query "${sql}" --split-by ${id} -m 10 \
    --target-dir ${hive_dir}/${hive_table} \
    --fields-terminated-by "\001" --lines-terminated-by "\n" \
    --null-string '\\N' --null-non-string '\\N'
}
#Author  : libin
#agrs 3  : jdbc_table, exec_sql, target_dir
#function: import_data_to_hdfs
function import_data_hdfs() {
  if [ $# -lt 5 ] ; then
    echo_ex "args count $# is error!"
    return 255
  fi
  exec_sql=$1
  target_dir=$2

  jdbc_url=$3
  jdbc_username=$4
  jdbc_passwd=$5
  id="id"
  if [ $# -gt 5 ] ; then
    id=$6
  fi
  echo_ex "$exec_sql"
  echo_ex "start import from jdbc_table..."
  if [ -n "${target_dir}" ] && [ ${target_dir} != "/" ]; then
    echo_ex "${HADOOP} fs -rmr ${target_dir}"
    ${HADOOP} fs -rmr "${target_dir}"
  fi
  echo_ex "-split-by ${id} -m 10"
  echo_ex "${target_dir}"
  sqoop import \
    --connect ${jdbc_url} --username ${jdbc_username} --password  ${jdbc_passwd} \
    --query "${exec_sql}" \
    --split-by ${id} -m 10 \
    --target-dir ${target_dir} \
    --fields-terminated-by "\001" --lines-terminated-by "\n" \
    --hive-drop-import-delims \
    --null-string '\\N' --null-non-string '\\N'
  check_success
  echo_ex "end successful import ${target_dir}. field.delim : \001"
}
#function : Hdfs To Mysql
function import_data_hdfs_to_mysql() {
  if [ $# -lt 5 ] ; then
    echo_ex "args count $# is error!"
    return 255
  fi
  jdbc_url=$1
  jdbc_username=$2
  jdbc_passwd=$3

  table=$4
  hdfs_path=$5

  sqoop export \
    --connect ${jdbc_url} --username ${jdbc_username} --password  ${jdbc_passwd} --table ${table} \
    --export-dir ${hdfs_path} \
    --input-fields-terminated-by '\t'
  check_success
  echo_ex "end successful export data into ${table} end !."
}
